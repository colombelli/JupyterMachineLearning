{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pet-ml2.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"DWkN_8Jlnwtn","colab_type":"text"},"cell_type":"markdown","source":["Document with the tasks and further explanation: https://docs.google.com/document/d/1aeidmS9pZ4CSHwLx08E79MT9Ch-h6EO6g2qesSC_wEY/edit#\n","\n","\n","\n","GitHub: [@colombelli](http://github.com/colombelli/)"]},{"metadata":{"id":"ex04WC04CU7w","colab_type":"code","colab":{}},"cell_type":"code","source":["from numpy import exp\n","\n","\n","class Neuron:\n","  def __init__(self, weights, bias):  # class constructor\n","    self.weights = weights\n","    self.bias = bias\n","    self.output = 0\n","\n","      \n","  def activation_function(self, input):  # used to strech or contract outputs between some range\n","    self.output = 1.0 / (1.0 + exp(-(input)))  # the sigmoid activation function normally used\n","    return self.output\n","    #return input  # in this case, there's no activation function since we just need the sum of the inputs: always performed in the z method (but summing the inputs with their weights)\n","      \n","\n","  def z(self, inputs):  # outputs the sum of every input times its respective weight, which are always 1 for this specific Task; and add a bias in the final result (0 in this particular Task)\n","    \n","    zOut = 0\n","    for i in range(len(inputs)):\n","      zOut += inputs[i] * self.weights[i]\n","    return zOut + self.bias  \n","\n","    \n","  def y(self, inputs):  # final output - neuron value when activated\n","    return self.activation_function(self.z(inputs))\n","  \n","\n","  def dEdz(self, dEdy):\n","  # assuming that our activation function is a sigmoid one, dEdz = dEdy * dydz   -> dEdz = dEdy * y * (1 - y) \n","    return dEdy * self.output * (1 - self.output)\n","\n","  \n","  def dEdw(self, dEdy, inputs):  # dEdw = dzdw * dydz * dEdy = dzdw * dEdz     , where dz(i)dw is simply the i input\n","    dEdz = self.dEdz(dEdy)\n","    dEdw = []\n","    \n","    for i, w in enumerate(self.weights):\n","      dEdw.append(inputs[i] * dEdz)\n","      \n","    return dEdw\n","    \n","  \n","  # we have to update the weights based on the learning rate and the derivatives\n","  def updateWeights(self, learningRate, dEdw):\n","    for i in range(len(self.weights)):\n","      self.weights[i] -= learningRate * dEdw[i]\n","          \n","  \n","  # the same goes to the bias\n","  def updateBias(self, learningRate, dEdz):\n","    self.bias -= learningRate * dEdz  # because dEdb = dzdz * dEdz = 1 * dEdz = dEdz"],"execution_count":0,"outputs":[]},{"metadata":{"id":"neXOoYm1LWLT","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","class DenseLayer:\n","    def __init__(self, num_of_inputs, num_of_neurons):\n","      self.num_of_inputs = num_of_inputs\n","      self.neurons = []\n","      \n","\n","      for i in range(num_of_neurons):  # creating the neurons for the layer\n","        weights = np.random.uniform(-1,1,[num_of_inputs])  # randomizing \"num_inputs\" weights with a value between 0 and 1\n","        #weights = np.ones((num_of_inputs,), dtype=int)  # generating a vector of ones to use in the Task 1 of summing up Xi inputs\n","        bias = np.random.uniform(0,1)  # randomizing a bias\n","        #bias = 0  # also, in order to use Task 1 and test Task 2 easier, the bias will be always zero\n","        self.neurons.append(Neuron(weights, bias))  # appending the new neuron to the layer\n","\n","\n","    def feedForward(self, inputs):  # activates every neuron in the layer, outputting their respestive results\n","      self.inputs = inputs  # save the inputs as an attribute in order to use it in the dzdw calculus\n","      lisOut = []\n","      for neuron in self.neurons:\n","        lisOut.append(neuron.y(inputs))\n","      return lisOut"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-v-l3XFNWRyi","colab_type":"code","colab":{}},"cell_type":"code","source":["class NeuralNetwork:\n","  def __init__(self, num_of_inputs, num_of_neurons_at_each_layer):\n","    self.num_of_inputs = num_of_inputs\n","    self.layers = []\n","    \n","    # creates a dense layer for every int on the list num_of_neurons_at_each_layer  \n","    num_of_inputs_next_neuron = num_of_inputs  # the first layer will have the given number of inputs for each of its neuron\n","    for num in num_of_neurons_at_each_layer:   \n","      self.layers.append(DenseLayer(num_of_inputs_next_neuron, num))\n","      num_of_inputs_next_neuron = num  # the next layers will have the number of neurons of the previous layer as its number of inputs\n","  \n","  \n","  def feedForward(self, inputs):\n","    \n","    for layer in self.layers:  # keeps picking the outputs of every layer and passing them as inputs to the next layer\n","      #print(\"TESTE\")\n","      #print(inputs)\n","      #input(\"tecla para continuar\")\n","      inputs = layer.feedForward(inputs)     \n","    \n","    # the outputs will be the final result of the inputs variable, after the end of the loop above\n","    outputs = inputs  # for code clarity, this variable is created before the method returns\n","      \n","    return outputs\n","  \n","  \n","  def derivative_of_the_error(self, value, result):\n","    # the derivative of the RSS with respect of each dimension  is: 2 * (dimensionResult - dimensionValue)\n","    # returns an array with the result of each value\n","  \n","    dEdy = []\n","    for i in range(len(value)):  # calculates each value of that derivative \n","      dEdy.append((result[i] - value[i]) * 2)\n","  \n","    return dEdy\n","  \n","  \n","  def backpropagation(self, value, result, learningRate):\n","    \n","    dEdy = self.derivative_of_the_error(value, result)  # calculates the first dEdy related to the final output\n","    \n","    flagFirstLayer = 1  # indicates that the dEdy is already calculated for that layer (the first starting at the end)\n","    for layer in reversed(self.layers):  # update the weights for each neuron in each layer (starting at the end)     \n","      \n","      new_dEdy = np.zeros(len(layer.neurons[0].weights))   \n","      \n","      for i, neuron in enumerate(layer.neurons):  # updates the neurons weights and biases in the layer\n","\n","        dEdw = neuron.dEdw(dEdy[i], layer.inputs)  # computates dEdw for given neuron\n","        dEdz = neuron.dEdz(dEdy[i])  # necessary to update the bias and for calculating the new dEdy used in the next iterable layer\n","        neuron.updateWeights(learningRate, dEdw)\n","        neuron.updateBias(learningRate, dEdz)\n","        \n","        # sums up the right weight multiplying by the dEdz in the right place of the array\n","        for w in range(len(neuron.weights)):\n","          new_dEdy[w] += neuron.weights[w] * dEdz\n","        \n","      # finally, with the new dEdy array constructed, we update it and  propagates it to next iterable layer\n","      dEdy = new_dEdy\n","          \n","       "],"execution_count":0,"outputs":[]},{"metadata":{"id":"Rulz552NZYRQ","colab_type":"code","colab":{}},"cell_type":"code","source":["def error_function(value, result):  # where the value represents what the network should output and result represents what it actually outputted\n","  \n","  RSS = 0\n","  for i in range(len(value)):\n","    RSS += (result[i]- value[i])**2\n","  \n","  return RSS"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Vk7pPbj5rfZm","colab_type":"code","colab":{}},"cell_type":"code","source":["def derivative_of_the_error(value, result):\n","  # the derivative of the RSS with respect of each dimension  is: 2 * (dimensionResult - dimensionValue)\n","  \n","  derivatives = []\n","  for i in range(len(value)):  # calculates each value of that derivative \n","    derivatives.append((result[i] - value[i]) * 2)\n","  \n","  return derivatives    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"lyJaZrnradDz","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import time\n","mnist = tf.keras.datasets.mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","\n","numInputs = 784\n","neuronsEachLayer = [30, 10]   \n","network = NeuralNetwork(numInputs, neuronsEachLayer)\n","learning_rate = 0.001\n","MAX_TRAINING_SAMPLES = 60000"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eTetP34wYbPA","colab_type":"code","outputId":"b48e8f17-4089-48e6-fecc-4e030ef1b8be","executionInfo":{"status":"ok","timestamp":1551163110397,"user_tz":180,"elapsed":8176279,"user":{"displayName":"F. Colombelli","photoUrl":"https://lh5.googleusercontent.com/-MwrIqIaB1b8/AAAAAAAAAAI/AAAAAAAAAXo/qL4E3eHesak/s64/photo.jpg","userId":"00031106970244302783"}},"colab":{"base_uri":"https://localhost:8080/","height":1071}},"cell_type":"code","source":["# Training network\n","\n","startTime = time.time()\n","sample_n = 0\n","matches = 0\n","\n","for sample, label in zip(x_train, y_train):\n","  \n","  sample_n += 1\n","  \n","  if sample_n == MAX_TRAINING_SAMPLES:\n","    time_taken = time.time() - startTime\n","    break\n","    \n","  if (sample_n % 1000) == 0:\n","    print(\"%d trained\" %sample_n)\n","    \n","  sample = np.concatenate(sample)\n","  \n","  # convert the output to one hot encoded\n","  expected_output = np.zeros(10)\n","  expected_output[label] = 1\n","  \n","  network_output = network.feedForward(sample)  # computes network output\n","  loss = error_function(expected_output, network_output)  # computes loss\n","  \n","  nn_guess = np.argmax(network_output)  # gets the index of the highest value in the array\n","  if nn_guess == label:  # check if the network got a right guess\n","    matches += 1\n","  \n","  \"\"\"\n","  \n","  NOTE: PRINTING THE RESULTS OF EVERY STEP WILL CAUSE THE BROWSER TO EVENTUALLY \n","        CRASH, THEN IT'S SAFER TO JUST RUN THE CODE WITH FEW OUTPUTS INFORMING \n","        THE PROGRESS OF THE PROCESSING.\n","  \n","  # print results\n","  print(\"Sample %d | Label = %d | Output = %d | %d matches\" % (sample_n, label, nn_guess, matches))\n","  print(\"Loss: \", loss)\n","  print(\"Time taken:\", time.time() - startTime)\n","  print(\"\\n\")\n","  \n","  \"\"\"\n","\n","  # update weights and biases\n","  network.backpropagation(expected_output, network_output, learning_rate)\n","\n","print(\"Time taken: \", time_taken)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: overflow encountered in exp\n","  if sys.path[0] == '':\n"],"name":"stderr"},{"output_type":"stream","text":["1000 trained\n","2000 trained\n","3000 trained\n","4000 trained\n","5000 trained\n","6000 trained\n","7000 trained\n","8000 trained\n","9000 trained\n","10000 trained\n","11000 trained\n","12000 trained\n","13000 trained\n","14000 trained\n","15000 trained\n","16000 trained\n","17000 trained\n","18000 trained\n","19000 trained\n","20000 trained\n","21000 trained\n","22000 trained\n","23000 trained\n","24000 trained\n","25000 trained\n","26000 trained\n","27000 trained\n","28000 trained\n","29000 trained\n","30000 trained\n","31000 trained\n","32000 trained\n","33000 trained\n","34000 trained\n","35000 trained\n","36000 trained\n","37000 trained\n","38000 trained\n","39000 trained\n","40000 trained\n","41000 trained\n","42000 trained\n","43000 trained\n","44000 trained\n","45000 trained\n","46000 trained\n","47000 trained\n","48000 trained\n","49000 trained\n","50000 trained\n","51000 trained\n","52000 trained\n","53000 trained\n","54000 trained\n","55000 trained\n","56000 trained\n","57000 trained\n","58000 trained\n","59000 trained\n","Time taken:  8175.438211917877\n"],"name":"stdout"}]},{"metadata":{"id":"j-76-X171PWl","colab_type":"code","outputId":"d1b2dd71-1937-4671-f7db-576b8636a3a4","executionInfo":{"status":"ok","timestamp":1551163444388,"user_tz":180,"elapsed":59133,"user":{"displayName":"F. Colombelli","photoUrl":"https://lh5.googleusercontent.com/-MwrIqIaB1b8/AAAAAAAAAAI/AAAAAAAAAXo/qL4E3eHesak/s64/photo.jpg","userId":"00031106970244302783"}},"colab":{"base_uri":"https://localhost:8080/","height":833}},"cell_type":"code","source":["# Testing network accuracy\n","\n","MAX_TEST_SAMPLES = 1000\n","sample_num = 0\n","matches_t = 0\n","\n","for sample, label in zip(x_test, y_test):\n","  \n","  sample_num += 1\n","  \n","  if sample_num == MAX_TEST_SAMPLES:\n","    break\n","    \n","  sample = np.concatenate(sample)\n","  \n","  # convert the output to one hot encoded\n","  expected_output = np.zeros(10)\n","  expected_output[label] = 1\n","  \n","  network_output = network.feedForward(sample)  # computes network output\n","  loss = error_function(expected_output, network_output)  # computes loss\n","  \n","  nn_guess = np.argmax(network_output)  # gets the index of the highest value in the array\n","  if nn_guess == label:  # check if the network got a right guess\n","    matches_t += 1\n","  \n","  if (sample_num % 100) == 0:  # prints parcial results from 100 to 100 tested samples\n","    # print results\n","    print(\"Sample %d | Label = %d | Output = %d | %d matches\" % (sample_num, label, nn_guess, matches_t))\n","    print(\"Loss: \", loss)\n","    print(\"Hit rate: %.2f%%\" % (matches_t / sample_num * 100))\n","    print(\"\\n\")\n","    \n","print(\"Final hit rate: %.2f%%\" % (matches_t / sample_num * 100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: overflow encountered in exp\n","  if sys.path[0] == '':\n"],"name":"stderr"},{"output_type":"stream","text":["Sample 100 | Label = 9 | Output = 9 | 44 matches\n","Loss:  0.39640316123746433\n","Hit rate: 44.00%\n","\n","\n","Sample 200 | Label = 2 | Output = 2 | 91 matches\n","Loss:  0.6563651219245351\n","Hit rate: 45.50%\n","\n","\n","Sample 300 | Label = 8 | Output = 6 | 137 matches\n","Loss:  1.2587315719002707\n","Hit rate: 45.67%\n","\n","\n","Sample 400 | Label = 4 | Output = 2 | 173 matches\n","Loss:  0.7920719544259793\n","Hit rate: 43.25%\n","\n","\n","Sample 500 | Label = 6 | Output = 8 | 211 matches\n","Loss:  0.9111671371042018\n","Hit rate: 42.20%\n","\n","\n","Sample 600 | Label = 9 | Output = 9 | 247 matches\n","Loss:  0.39640316123746433\n","Hit rate: 41.17%\n","\n","\n","Sample 700 | Label = 3 | Output = 6 | 287 matches\n","Loss:  1.4097698613520893\n","Hit rate: 41.00%\n","\n","\n","Sample 800 | Label = 2 | Output = 2 | 327 matches\n","Loss:  0.4490350750493641\n","Hit rate: 40.88%\n","\n","\n","Sample 900 | Label = 8 | Output = 7 | 369 matches\n","Loss:  1.027200024911953\n","Hit rate: 41.00%\n","\n","\n","Final hit rate: 40.80%\n"],"name":"stdout"}]},{"metadata":{"id":"QuIVogrpAYiR","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}